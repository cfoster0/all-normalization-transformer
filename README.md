## Transformer with Normalized Attention (wip)

A Transformer that consists of only normalization as its sole non-linearity, as proposed in the paper <a href="https://arxiv.org/abs/2005.09561">Normalized Attention Without Probability Cage</a>. This repository will build on the paper's contributions and attempt to make it work for the auto-regressive case.

## Citations

```bibtex
@misc{richter2020normalized,
    title={Normalized Attention Without Probability Cage},
    author={Oliver Richter and Roger Wattenhofer},
    year={2020},
    eprint={2005.09561},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```
