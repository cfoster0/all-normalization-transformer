## Transformer with Normalized Attention (wip)

A Transformer that consists of only normalization as its non-linearity, as proposed in <a href="https://arxiv.org/abs/2005.09561">this paper</a>. This repository will build on this and attempt to make this work for auto-regressive scenario. Experiments pending.

## Citations

```bibtex
@misc{richter2020normalized,
    title={Normalized Attention Without Probability Cage},
    author={Oliver Richter and Roger Wattenhofer},
    year={2020},
    eprint={2005.09561},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
```
